Assignments for [Berkeley CS 285: Deep Reinforcement Learning, Decision Making, and Control](http://rail.eecs.berkeley.edu/deeprlcourse/).

**Note**: The implementations work fine for tested environments, which means there may have bugs not detected.
The same can be suitable for **Analysis** part, especially mathematical derivations. If you find error(s), please let me know, I will be grateful.

## Update (Nov 7th, 2024)

I should stop temporarily and rethink my learning track after being stuck on model-free RL methods and theory for almost 2 weeks. I realized that RL learning is quite more challenging than other ML learning methods (e.g. supervised learning). It's more than model-free RL methods, I found I lack a lot of understanding of RL algorithms and experience to do the RL experiments. So I decided to start over and spend more time to understand essential algorithms (e.g. DQN, A2C, PPO). I believe some folks may also encounter the same situation as me. These 2 resources helped me a lot:

- [OpenAI Spinning Up as a Deep RL Researcher](https://spinningup.openai.com/en/latest/spinningup/spinningup.html)
- [Stable-Baselines3 Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)
